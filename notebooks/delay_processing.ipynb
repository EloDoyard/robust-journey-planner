{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delay processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will process the timetable data containing scheduled and actual data of transport throught Switzerland. We will use this data to compute average delays for each hour of the day, depending on the line and type of transport. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "get_ipython().run_cell_magic('configure', \n",
    "                             line=\"-f\", \n",
    "                             cell='{ \"name\":\"%s-project\", \"executorMemory\":\"4G\", \"executorCores\":4, \"numExecutors\":10, \"driverMemory\": \"4G\" }' % username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from hdfs\n",
    "df = spark.read.orc(\"/data/sbb/orc/istdaten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"hdfs:///user/theAggregators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the processed vertices to get the stop ids\n",
    "df_nodes = spark.read.csv(\"{}/vertices.csv\".format(folder_name), header=True).select(col(\"stop_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep node base prefix and drop platform information \n",
    "nodes = [n.stop_id.split(\":\")[0] for n in df_nodes.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the delays for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to only take into consideration delay that are positives: if the transport arrives in advance, we set the delay to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF that clips delay to zero if it is negative\n",
    "clip_neg = udf(lambda delay: 0 if delay < 0 else delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a filtered dataframe in order to find all the transports where: \n",
    "- The actual and scheduled arrival time are given.\n",
    "- The trip is not an additional trip and it did not fail.\n",
    "- The stop is in the list of stops that we should consider (inside 15-km radius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe according to the criteria above\n",
    "df_filtered = df.filter((df.ankunftszeit != \"\") & \\\n",
    "                       (df.an_prognose != \"\") & \\\n",
    "                       (df.zusatzfahrt_tf == \"false\") & \\\n",
    "                       (df.faellt_aus_tf == \"false\") & \\\n",
    "                       (df.bpuic.isin(nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the filtered table, we only need:\n",
    "- The name of the stop\n",
    "- The operation id (betreiber_id) and the line id (linen_id) to identify a trip\n",
    "- The id of the stop in the trip (bpuic) \n",
    "- The hour of the day during which the trip occurred, because we need a different lambda for each hour since the trafic can differ during the days\n",
    "\n",
    "We also compute the delay by calculating the difference between the actual arrival time (`an_prognose`) and the scheduled arrival time (`ankunftszeit`).\n",
    "\n",
    "Note that all delays are in seconds. The operator, line, stop id and the hour can help us compute the delay for each stop for a specific line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select fields of interest and compute delay in seconds. Keep hours between 6 am and 9 pm.   \n",
    "df_delay = df_filtered.select(\n",
    "    df_filtered.haltestellen_name,\n",
    "    df_filtered.betreiber_id, \n",
    "    df_filtered.linien_id, \n",
    "    df_filtered.bpuic,\n",
    "    hour(to_timestamp(col(\"ankunftszeit\"), \"dd.MM.yyyy HH:mm\")).alias(\"hour\"),\n",
    "    clip_neg((to_timestamp(col(\"an_prognose\"), \"dd.MM.yyyy HH:mm:ss\").cast(\"long\") \\\n",
    "                                - to_timestamp(col(\"ankunftszeit\"), \"dd.MM.yyyy HH:mm\").cast(\"long\"))) \\\n",
    "                                 .alias(\"delay\")\n",
    "    )\\\n",
    "    .filter((col(\"hour\") >= 6) & (col(\"hour\") < 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the mean delay per hour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe is ready, we can groupy by the stop name, the operator, stop id and the hour to find the average delay for a specific stop from a line at a given time during the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by stop name, line operator, stop id and hour\n",
    "# Compute mean delay for each group\n",
    "df_stop_line_delay = df_delay.groupBy(df_filtered.bpuic, df_delay.hour)\\\n",
    "                                .agg({\"delay\" : \"mean\"}).withColumnRenamed(\"avg(delay)\", \"delay\")\\\n",
    "                                .select(\n",
    "                                    col(\"bpuic\"),\n",
    "                                    col(\"hour\"),\n",
    "                                    col(\"delay\")\n",
    "                                )\\\n",
    "                                .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_line_delay.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many of the original stops our delay information covers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tuples we have delay info for \n",
    "df_stop_line_delay.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes our delay information covers\n",
    "df_stop_line_delay.select(col(\"bpuic\")).distinct().filter(col(\"bpuic\").isin(nodes)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes we are considering\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have delay parameters for 22970 tuples (bpuic, hour, delay) covering 1483 stops out of the 2306 that we are considering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding delay information to the list of edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we computed the average delay per stop, line, and hour, we will add this information to the edges of the graph we previously constructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the previous result into a pandas df\n",
    "delays = df_stop_line_delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the edges \n",
    "edges = spark.read.csv(\"{}/edges.csv\".format(folder_name), header=True).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert some fields to their correct type, to avoid mistakes later on\n",
    "delays = delays.astype({'bpuic' : str, 'hour': int})\n",
    "edges = edges.astype({'dst' : str, 'src': str, 'agency_id': str, 'route_short_name': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column corresponding to the hour of the day\n",
    "edges['hour'] = edges.dst_arrival.apply(lambda x: int(x.split(':')[0]))\n",
    "# Parse the destination stop ids to ignore the platform number\n",
    "edges[\"simple_dst_id\"] = edges.dst.apply(lambda x: x.split(\":\")[0])\n",
    "\n",
    "# Truncate the bpuic to have the same format as in edges\n",
    "delays[\"len_bpuic\"] = delays.bpuic.apply(lambda x: len(x))\n",
    "delays.loc[delays.len_bpuic == 9, \"bpuic\"] = delays.loc[delays.len_bpuic == 9, \"bpuic\"].apply(lambda x : x[:-2])\n",
    "delays = delays.drop(columns=\"len_bpuic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the two dataframes, to associate the destination of every edge to a delay: i.e the average delay that the train will have when arriving at the end of the edge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes on the stop_id and hour of the day, to add delay information to each edge\n",
    "combined_df = edges.merge(delays,left_on=['simple_dst_id','hour'],right_on=['bpuic','hour'],how=\"left\")\\\n",
    "                    .drop_duplicates(subset=[\"src\", \"dst\", \"src_departure\", \"dst_arrival\", \"trip_id\"])\\\n",
    "                    .drop(columns=[\"bpuic\", \"simple_dst_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer delay information for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the edges might have not been matched with a delay. This is because we do not have delay information for all our nodes. To address that problem, we fill these missing delays with the average delay of lines from the same mean of transport. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transport types to more general names\n",
    "to_generic_transport = {\n",
    "    \"S-Bahn\": \"Train\",\n",
    "    \"Bus\": \"Bus\",\n",
    "    \"Tram\": \"Tram\",\n",
    "    \"Standseilbahn\": \"Other\", \n",
    "    \"Schiff\": \"Other\",\n",
    "    \"Luftseilbahn\": \"Other\",\n",
    "    \"TGV\": \"Train\",\n",
    "    \"Taxi\": \"Other\",\n",
    "    \"Eurocity\": \"Train\",\n",
    "    \"InterRegio\": \"Train\",\n",
    "    \"ICE\": \"Train\",\n",
    "    \"Intercity\": \"Train\",\n",
    "    \"RegioExpress\": \"Train\",\n",
    "    \"InterRegio\": \"Train\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the more general transport type \n",
    "combined_df[\"type\"] = combined_df.route_desc.apply(lambda x : to_generic_transport[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average delay per hour and per general transport type\n",
    "avg_delay_hour = combined_df.loc[:,(\"hour\", \"delay\", \"type\")].groupby([\"hour\", \"type\"]).mean()\n",
    "delay_dict = avg_delay_hour.to_dict()[\"delay\"]\n",
    "\n",
    "# Fill missing delay values with average of corresponding transport type\n",
    "combined_df.delay = combined_df.apply(lambda x : delay_dict[(x.hour, x.type)] if pd.isna(x.delay) else x.delay, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the general type: we don't need it anymore\n",
    "new_edges = combined_df.drop(columns=\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated edges on hdfs\n",
    "spark.createDataFrame(new_edges).write.mode(\"overwrite\").option(\"compression\",\"gzip\").option(\"header\", \"True\").csv(\"{}/delays.csv\".format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_line_delay.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
