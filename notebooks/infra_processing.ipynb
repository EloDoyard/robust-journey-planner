{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure processing\n",
    "\n",
    "In this notebook, we prepare the data using which we will build our network. Using the timetable data available, we prepare the edges and vertices required to build that graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from itertools import combinations\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "namespace = os.environ['CI_NAMESPACE']\n",
    "project = os.environ['CI_PROJECT']\n",
    "\n",
    "configuration = dict(\n",
    "    name = f\"{username}-{namespace}-{project}\",\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4,\n",
    "    numExecutors = 10,\n",
    "    conf = {\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "    })\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.run_cell_magic('configure', line=\"-f\", cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint folder\n",
    "checkpoint = 'hdfs:///user/{}/checkpoint/'.format(username)\n",
    "print('checkpoint created at hdfs:///user/{}/checkpoint/'.format(username))\n",
    "sc.setCheckpointDir(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with train services "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep services that run on every business day, i.e. Monday through Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read calendar data\n",
    "calendar = spark.read.csv(\"/data/sbb/csv/timetable/calendar/2019/05/07/calendar.csv\",  header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter calendar to keep services that run on all 5 business days\n",
    "business_day_calendar = calendar.filter((col(\"monday\") == 1) & \\\n",
    "                                        (col(\"tuesday\") == 1) & \\\n",
    "                                        (col(\"wednesday\") == 1) & \\\n",
    "                                        (col(\"thursday\") == 1) & \\\n",
    "                                        (col(\"friday\") == 1)).select(\"service_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only the trips whose `service_id` is in the list computed above, i.e. trips that run on typical business day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trips data\n",
    "trips = spark.read.csv(\"/data/sbb/csv/timetable/trips/2019/05/07/trips.csv\",  header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the trips that run during the services (i.e. business days)\n",
    "business_day_trips = business_day_calendar.join(trips, on = \"service_id\", how = \"left_outer\").drop(\"service_id\", \"direction_id\")\n",
    "business_day_trips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the route information to the dataframe, i.e. the `agency_id`, the `route_short_name` and the `route_desc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the routes data\n",
    "routes = spark.read.csv(\"/data/sbb/csv/timetable/routes/2019/05/07/routes.csv\",  header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the trips data with information about the route (agency, route short name and description)\n",
    "business_day_routes = business_day_trips.join(routes, on = \"route_id\", how = \"left_outer\").drop(\"route_long_name\", \"route_type\")\n",
    "business_day_routes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with stop times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the stop times in the dataframe, but we only keep trips (or part of trips) that run after 6 a.m. and arrive before 9 p.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stop times data\n",
    "stop_times = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\",  header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment this data with information about trips and stops\n",
    "business_day_stop_times = business_day_routes.join(stop_times, on = \"trip_id\", how = \"left_outer\").drop(\"pickup_type\", \"drop_off_type\")\n",
    "business_day_stop_times.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep stop times occuring during \"reasonable\" business day hours\n",
    "business_day_stop_times = business_day_stop_times.filter( (col(\"departure_time\") >= \"06:00:00\") & (col(\"arrival_time\") < \"21:00:00\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load information about the transportation stops. We will filter these stops to retain those inside a 15 km radius around Zurich's main station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stops data\n",
    "stops = spark.read.orc(\"/data/sbb/orc/geostops/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the exact location of Zurich HB and broadcasting it\n",
    "zurich_HB_location = stops.filter(stops.stop_id == '8503000').select('stop_lat', 'stop_lon').first()\n",
    "bc_zhb_location = spark.sparkContext.broadcast(zurich_HB_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def haversine(lat1, lon1, lat2 = bc_zhb_location.value.stop_lat, lon2 = bc_zhb_location.value.stop_lon):\n",
    "    '''\n",
    "        Returns the distance between two points in Km\n",
    "    '''\n",
    "    R = 6372.8\n",
    "    dLat = radians(lat2 - lat1)\n",
    "    dLon = radians(lon2 - lon1)\n",
    "    lat1 = radians(lat1)\n",
    "    lat2 = radians(lat2)\n",
    "\n",
    "    a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
    "    c = 2*asin(sqrt(a))\n",
    "\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering stops within a 15 km radius from Zurich HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The geostops dataset filtered to include only the stops that are whithin a 15km radius of ZÃ¼rich Hauptbahnhof\n",
    "stops_15_from_zhb = stops.filter(haversine(stops.stop_lat, stops.stop_lon) <= 15.0)\n",
    "\n",
    "# An broadcast object containing the stop_ids of all the stops in this area\n",
    "bc_stops_15_from_zhb = spark.sparkContext.broadcast([row.stop_id for row in stops_15_from_zhb.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def trip_in_15_zhb(stop_list):\n",
    "    '''\n",
    "        Checks if the list of stops contains at least one \n",
    "        stop from the stops that are in the 15-km radius\n",
    "        around Zurich HB\n",
    "    '''\n",
    "    return any([stop in bc_stops_15_from_zhb.value for stop in stop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a list of stops, we retain trips that have at least one stop contained in the 15-km radius around Zurich HB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We group by trip_id and we store in a list all the stops of a trip_id, \n",
    "# along with the departure and arrival time of each stop.\n",
    "# We keep only the trips where at least one stop is at less than 15 km from Zurich HB\n",
    "trip_stops_within_15 = business_day_stop_times.groupby('trip_id', 'trip_short_name', 'agency_id', \"route_short_name\", 'route_desc', 'route_id')\\\n",
    "                                                    .agg(F.collect_list(\"stop_sequence\").alias(\"sequence\"), \n",
    "                                                         F.collect_list(\"stop_id\").alias(\"stops\"), \n",
    "                                                         F.collect_list(\"arrival_time\").alias(\"arrival_times\"),\n",
    "                                                         F.collect_list(\"departure_time\").alias(\"departure_times\"))\\\n",
    "                                                    .filter(trip_in_15_zhb(\"stops\") == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_stops_within_15.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the underlying graph\n",
    "\n",
    "Now that we have the required filtered information, we can proceed to prepare the edges for our graph. We decided to have an edge between consecutive stops of a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pairs of consecutive stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(ArrayType(ArrayType(StringType())))\n",
    "def make_stop_pairs(stop_sequence, stop_list, departures_list, arrivals_list):\n",
    "    '''\n",
    "        Given a list of stop ids, creates a list of tuples\n",
    "        of consecutive stops\n",
    "    '''\n",
    "    quadruples = []\n",
    "    stop_list_sorted = [x for _, x in sorted(zip(stop_sequence, stop_list), key=lambda pair: pair[0])]\n",
    "    departures_list_sorted = [x for _, x in sorted(zip(stop_sequence, departures_list), key=lambda pair: pair[0])]\n",
    "    arrivals_list_sorted = [x for _, x in sorted(zip(stop_sequence, arrivals_list), key=lambda pair: pair[0])]\n",
    "\n",
    "    for i in range(1, len(stop_list)):\n",
    "        src, dst = stop_list_sorted[i-1], stop_list_sorted[i]\n",
    "        src_departure = departures_list_sorted[i-1]\n",
    "        dst_arrival = arrivals_list_sorted[i]\n",
    "        quadruples.append([src, dst, src_departure, dst_arrival])\n",
    "     \n",
    "    return quadruples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe containing the edges of our network\n",
    "# there is an edge between 2 stops if there exists at least one trip passing through both stops\n",
    "edges = trip_stops_within_15.withColumn('edge', \n",
    "                                        F.explode(make_stop_pairs(trip_stops_within_15.sequence, trip_stops_within_15.stops, \n",
    "                                                                  trip_stops_within_15.departure_times, trip_stops_within_15.arrival_times))\n",
    "                                       )\\\n",
    "                            .select(col('edge')[0].alias('src'), \n",
    "                                    col('edge')[1].alias('dst'), \n",
    "                                    col('edge')[2].alias('src_departure'), \n",
    "                                    col('edge')[3].alias('dst_arrival'),\n",
    "                                    col('trip_id'), \n",
    "                                    col('trip_short_name'), \n",
    "                                    col('agency_id'), \n",
    "                                    col('route_short_name'), \n",
    "                                    col('route_desc'),\n",
    "                                   col('route_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now drop duplicates to keep one edge per unique source, destination, arrival and departure times, and route id (i.e. unique identifier for a line of transport). This is avoid having multiple for the same trip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate edges with regard to the source, destination, arrival and departure times\n",
    "edges = edges.dropDuplicates(['src', 'dst', 'src_departure', 'dst_arrival', 'route_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding transfer edges (i.e. walking edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the edges we just created, we build the list of vertices of our graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the edges, we extract the stop that needs to be considered for transfers\n",
    "new_stops = edges.select(col('src').alias('stop_id'))\\\n",
    "                 .union(edges.select(col('dst').alias('stop_id'))).distinct()\\\n",
    "                 .join(stops, on = 'stop_id', how = 'left_outer').cache()\n",
    "\n",
    "# Copy to be able to perform cross join\n",
    "new_stops_bis = new_stops.select(col(\"stop_id\").alias(\"stop_id_bis\"), col(\"stop_lat\").alias(\"stop_lat_bis\"), col(\"stop_lon\").alias(\"stop_lon_bis\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create pairs of vertices and keep those that are less than 500m apart. These pairs will correspond to what we call \"transfer edges\", i.e walking edges that correspond to transfers between closeby stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create walking edges dataframe containing all the stops between which there is less than 500 m \n",
    "walking_edges = new_stops.crossJoin(new_stops_bis)\\\n",
    "                                 .withColumn('distance', haversine(col('stop_lat'), col('stop_lon'), \n",
    "                                                                   col('stop_lat_bis'), col('stop_lon_bis'))) \\\n",
    "                                 .filter((col('distance') > 0.0) & (col('distance') <= 0.5))\\\n",
    "                                 .select(col('stop_id').alias('src'), col('stop_id_bis').alias('dst'), 'distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the results on hdfs under our group user folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"hdfs:///user/theAggregators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stops.write.mode(\"overwrite\").option(\"compression\",\"gzip\").option(\"header\", \"True\").csv(\"{}/vertices.csv\".format(folder_name))\n",
    "edges.write.mode(\"overwrite\").option(\"compression\",\"gzip\").option(\"header\", \"True\").csv(\"{}/edges.csv\".format(folder_name))\n",
    "walking_edges.write.mode(\"overwrite\").option(\"compression\",\"gzip\").option(\"header\", \"True\").csv(\"{}/transfers.csv\".format(folder_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
