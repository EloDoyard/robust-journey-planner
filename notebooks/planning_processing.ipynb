{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning processing\n",
    "\n",
    "In this notebook, we process the edges and vertices previously generated into a `networkx` graph that we can handle locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from hdfs3 import HDFileSystem\n",
    "from time_helpers import compute_delta_time\n",
    "from planning_helpers import is_transfer_edge\n",
    "\n",
    "hdfs = HDFileSystem()\n",
    "group_path = \"/user/theAggregators/\"\n",
    "\n",
    "SPEED = 0.05/60 # km/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data from our group folder on hdfs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_hdfs(file_path):\n",
    "    '''\n",
    "        Reads data from the hdfs file system \n",
    "        and returns a dataframe corresponding to it\n",
    "    '''\n",
    "    result = pd.DataFrame()\n",
    "    for path in hdfs.ls(file_path):\n",
    "        if not \"SUCCESS\" in path:\n",
    "            with hdfs.open(path) as f:\n",
    "                result = result.append(pd.read_csv(f, compression='gzip'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the vertice data\n",
    "vertices = read_from_hdfs(f\"{group_path}vertices.csv\").drop(columns = ['location_type', 'parent_station']).set_index('stop_id')\n",
    "vertices.index = vertices.index.astype(str)\n",
    "\n",
    "# Read the edge data\n",
    "edges = read_from_hdfs(f\"{group_path}delays.csv\")\n",
    "\n",
    "# Read the transfers data\n",
    "transfers = read_from_hdfs(f\"{group_path}transfers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the networkx graph from the main edge list. We keep the attributes since they will come in useful during the shortest paths search. The graph is a multi-directed, meaning there can be multiple edges between two points, which is what weed need in this case: there might be multiple ways to go from A to B in a city. \n",
    "\n",
    "We also add the transfer edges with a route id called `walking`. We set the nodes attributes to contain their localisation and name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the multi-directed graph from the edgelist, retain edge attributes\n",
    "G = nx.from_pandas_edgelist(edges, source = 'src', target = \"dst\", edge_attr = True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "# Add transfer edges to the graph\n",
    "for idx, row in transfers.iterrows():\n",
    "    G.add_edge(str(row.src), str(row.dst), distance = row.distance, route_id = \"walking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set nodes attribute that will help display the nodes\n",
    "nx.set_node_attributes(G, vertices['stop_name'].to_dict(), 'name')\n",
    "nx.set_node_attributes(G, vertices['stop_lat'].to_dict(), 'lat')\n",
    "nx.set_node_attributes(G, vertices['stop_lon'].to_dict(), 'lon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the edges, we compute the time required to traverse it. For transport edges, it is the time at arrival minus the time at departure. For transfer edges, it is the distance divided by the walking speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the time to travel through an edge\n",
    "for u, v, i, data in G.edges(data = True, keys = True):\n",
    "     # Regular transit edge\n",
    "    if not is_transfer_edge(G, (u, v, i)):\n",
    "        t = compute_delta_time(data['src_departure'], data['dst_arrival'])\n",
    "        nx.set_edge_attributes(G, {(u, v, i): t}, 'travel_time')\n",
    "    \n",
    "    # Transfer edge\n",
    "    else:\n",
    "        nx.set_edge_attributes(G, {(u, v, i): data['distance']/SPEED}, 'travel_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A directed graph is strongly connected if and only if every vertex in the graph is reachable from every other vertex. We check that it is the case, and that Zurich HB is in the largest strongly conected component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Is the graph strongly connected ? {}'.format(nx.is_strongly_connected(G)))\n",
    "print('There are {} strongly connected components in the graph.'.format(nx.number_strongly_connected_components(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the sizes of all connected components\n",
    "for i, cc in enumerate(nx.strongly_connected_components(G)):\n",
    "    print('Component {} has size {}.'.format(i + 1, len(cc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take largest connected components and verify that Zurich HB is in it\n",
    "largest_cc = sorted(nx.strongly_connected_components(G), key=len, reverse=True)[0]\n",
    "for node in largest_cc:\n",
    "    if '8503000' in node:\n",
    "        print('Zurich is in the largest connected component.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subgraph corresponding to largest cc\n",
    "G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "# Save the graph locally (lfs) \n",
    "nx.write_gpickle(G, '../data/graph.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
